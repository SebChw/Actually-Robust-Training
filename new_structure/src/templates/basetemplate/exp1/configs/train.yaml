defaults:
  - _self_
  - datamodule: sdx_labelnoise
  - module: source_separation
  - trainer: default
  - logger: neptune

# task name, determines output directory path
task_name: "wake-word-detection"

#Flag allowing another sanity check -> You can easily calculate expected value of the loss when you initialize your model with random weights.
validate_loss_at_init: True

#Flag allowing sanity check -> If your implementation can't even overfit one batch, then it's probably something wrong.
overfit_one_batch: False

# set False to skip model training
train: True

# evaluate on test set, using best model weights achieved during training
# lightning chooses best weights based on the metric specified in checkpoint callback
test: True

# simply provide checkpoint path to resume training
ckpt_path: null

# seed for random number generators in pytorch, numpy and python.random
seed: 42

#Whether to use torch 2.0 compile function
compile: False

upload_best_model: True

continue_training_id: null

#Lightning checkpointing definition
checkpoints:
  _target_: lightning.pytorch.callbacks.ModelCheckpoint
  save_top_k: 1
  monitor: VALIDATION_acc
  mode: max
  dirpath: checkpoints
  filename: "${task_name}-{epoch}-{val_loss:.2f}"
